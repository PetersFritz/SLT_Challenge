{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "from gensim.models import KeyedVectors\n",
        "word_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)"
      ],
      "metadata": {
        "id": "CU57cvElRwdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-geometric\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import cross_entropy\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, r2_score, mean_squared_error\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "WTz7VpJcWx_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_graph_info(nodes_degree_txt, nodes_edges_txt):\n",
        "  nodes_features = defaultdict(dict)\n",
        "  edges = []\n",
        "  with open(nodes_degree_txt, 'r') as nodes_degree_file, open(nodes_edges_txt, 'r') as nodes_edges_file:\n",
        "      reading_nodes, reading_edges = False, False\n",
        "\n",
        "      for line in nodes_edges_file:\n",
        "          line = line.strip()\n",
        "          if (not line) or line.startswith(\"id*int\") or line.startswith(\"source*int\"):\n",
        "              continue\n",
        "\n",
        "          if line.startswith(\"*Nodes\"):\n",
        "              # start reading node info\n",
        "              reading_nodes = True\n",
        "              reading_edges = False\n",
        "          elif line.startswith(\"*DirectedEdges\"):\n",
        "              # start reading edge info\n",
        "              reading_nodes = False\n",
        "              reading_edges = True\n",
        "\n",
        "          elif reading_nodes:\n",
        "            # parse node info\n",
        "              parts = line.split(\" \", 2)\n",
        "              node_id = int(parts[0])\n",
        "              label = parts[1].strip('\"')\n",
        "              nodes_features[node_id]['label'] = label\n",
        "\n",
        "\n",
        "          elif reading_edges:\n",
        "            # parse edge info\n",
        "              parts = line.split()\n",
        "              source = int(parts[0])\n",
        "              target = int(parts[1])\n",
        "              edges.append((source, target))\n",
        "\n",
        "      for line in nodes_degree_file:\n",
        "        line = line.strip()\n",
        "        if (not line) or (not line.startswith(\"Node Degree\")):\n",
        "          parts = line.split()\n",
        "          node_id = int(parts[0])\n",
        "          degree = parts[1].strip('\"')\n",
        "          nodes_features[node_id]['degree'] = degree\n",
        "\n",
        "      # print(len(nodes_features), len(edges))\n",
        "      return nodes_features, edges\n",
        "\n",
        "\n",
        "def get_word_embedding(word, embedding_dim=50):\n",
        "    try:\n",
        "        return torch.tensor(word_vectors[word], dtype=torch.float)\n",
        "    except KeyError:\n",
        "        # for words not in the vocabulary, use a zero vector\n",
        "        print('WARNING: word not in the vocabulary: ', word)\n",
        "        return torch.zeros(embedding_dim)\n",
        "\n",
        "\n",
        "def prep_data_for_gnn(nodes_features, edges):\n",
        "    # mapping from node ID to index\n",
        "    node_id_to_index = {node_id: i for i, node_id in enumerate(nodes_features.keys())}\n",
        "\n",
        "    # create node features: node degrees and word embeddings)\n",
        "    features = []\n",
        "    for node_id in nodes_features.keys():\n",
        "        node_label = nodes_features[node_id]['label']\n",
        "        node_embeddings = get_word_embedding(node_label)\n",
        "        features.append(node_embeddings)\n",
        "    x = torch.stack(features)  # Node feature matrix\n",
        "\n",
        "    # create edge index tensor\n",
        "    edge_index = torch.tensor(\n",
        "        [[node_id_to_index[src], node_id_to_index[dst]] for src, dst in edges],\n",
        "        dtype=torch.long,\n",
        "    ).t().contiguous()\n",
        "    return x, edge_index\n",
        "\n",
        "\n",
        "def get_dataloader(id_label_list,transcript_folder, batch_size=None):\n",
        "  \"\"\"\n",
        "  returns a list of Data objects\n",
        "  \"\"\"\n",
        "  if not batch_size:\n",
        "    batch_size = len(id_label_list)\n",
        "  data_list = []\n",
        "  for id, label in id_label_list:\n",
        "    filepath = os.path.join(transcript_folder, id)\n",
        "    nodes_features, edges = load_graph_info(f'{filepath}_dir_nodes_degree.txt', f'{filepath}_dir_nodes_edges.txt')\n",
        "    x, edge_index = prep_data_for_gnn(nodes_features, edges)\n",
        "    graph_data = Data(x=x, edge_index=edge_index, y=LABEL_MAPPING[label])\n",
        "    data_list.append(graph_data)\n",
        "  return DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "def calc_metrics(actual_labels, pred_vals):\n",
        "  results = {}\n",
        "  results['accuracy'] = accuracy_score(pred, batch.y)\n",
        "  results['f1'] = f1_score(actual_labels, pred_vals, average='macro')\n",
        "  results['precision'] = precision_score(actual_labels, pred_vals, average='macro')\n",
        "  results['recall'] = recall_score(actual_labels, pred_vals, average='macro')\n",
        "  results['confusion_matrix'] = confusion_matrix(actual_labels, pred_vals)\n",
        "  return results"
      ],
      "metadata": {
        "id": "q_bg9DDdUdat"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_MAPPING = {'HC': 0,\n",
        "                 'MCI': 1,\n",
        "                 'Dementia': 2}\n",
        "\n",
        "metadata = pd.read_csv(\"PROCESS_METADATA_ALL.csv\")\n",
        "df_train = metadata[metadata['Tr/Tt/Dv']=='train']\n",
        "df_dev = metadata[metadata['Tr/Tt/Dv']=='dev']\n",
        "df_test = metadata[metadata['Tr/Tt/Dv']=='test']\n",
        "train_id_label_list = [(id, label) for id, label in zip(df_train['anyon_IDs'], df_train['diagnosis'])]\n",
        "dev_id_label_list = [(id, label) for id, label in zip(df_dev['anyon_IDs'], df_dev['diagnosis'])]\n",
        "test_id_label_list = [(id, label) for id, label in zip(df_test['anyon_IDs'], df_test['diagnosis'])]\n",
        "\n",
        "transcript_folder = \"SFT_transcripts_2\"\n",
        "train_dataloader = get_dataloader(train_id_label_list, transcript_folder)\n",
        "dev_dataloader = get_dataloader(dev_id_label_list, transcript_folder)\n",
        "test_dataloader = get_dataloader(test_id_label_list, transcript_folder)"
      ],
      "metadata": {
        "id": "6GNWo04qEOyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b18efa-6a77-4f41-9514-be7f580d608d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: word not in the vocabulary:  greenfly\n",
            "WARNING: word not in the vocabulary:  panalito\n",
            "WARNING: word not in the vocabulary:  panadillo\n",
            "WARNING: word not in the vocabulary:  womba\n",
            "WARNING: word not in the vocabulary:  gazewa\n",
            "WARNING: word not in the vocabulary:  eurgh\n",
            "WARNING: word not in the vocabulary:  youve\n",
            "WARNING: word not in the vocabulary:  dedededededede\n",
            "WARNING: word not in the vocabulary:  mmmmmm\n",
            "WARNING: word not in the vocabulary:  thirtyfour\n",
            "WARNING: word not in the vocabulary:  thirtytwo\n",
            "WARNING: word not in the vocabulary:  naardvark\n",
            "WARNING: word not in the vocabulary:  chaffinches\n",
            "WARNING: word not in the vocabulary:  mmmmmmm\n",
            "WARNING: word not in the vocabulary:  youve\n",
            "WARNING: word not in the vocabulary:  dedededu\n",
            "WARNING: word not in the vocabulary:  dededededede\n",
            "WARNING: word not in the vocabulary:  lalalalala\n",
            "WARNING: word not in the vocabulary:  lalalalalala\n",
            "WARNING: word not in the vocabulary:  tetete\n",
            "WARNING: word not in the vocabulary:  chchchch\n",
            "WARNING: word not in the vocabulary:  sealions\n",
            "WARNING: word not in the vocabulary:  badumbadumbadum\n",
            "WARNING: word not in the vocabulary:  itll\n",
            "WARNING: word not in the vocabulary:  dikdik\n",
            "WARNING: word not in the vocabulary:  squir\n",
            "WARNING: word not in the vocabulary:  slowworm\n",
            "WARNING: word not in the vocabulary:  chchchchch\n",
            "WARNING: word not in the vocabulary:  chchch\n",
            "WARNING: word not in the vocabulary:  foodrelated\n",
            "WARNING: word not in the vocabulary:  bebebebebebebe\n",
            "WARNING: word not in the vocabulary:  phphhtphht\n",
            "WARNING: word not in the vocabulary:  dikdik\n",
            "WARNING: word not in the vocabulary:  youve\n",
            "WARNING: word not in the vocabulary:  garoo\n",
            "WARNING: word not in the vocabulary:  youve\n",
            "WARNING: word not in the vocabulary:  dikdik\n",
            "WARNING: word not in the vocabulary:  kongoni\n",
            "WARNING: word not in the vocabulary:  bleurgheurgheurgh\n",
            "WARNING: word not in the vocabulary:  twentyfour\n",
            "WARNING: word not in the vocabulary:  phhtphhtphhtphhtphht\n",
            "WARNING: word not in the vocabulary:  errrr\n",
            "WARNING: word not in the vocabulary:  chchch\n",
            "WARNING: word not in the vocabulary:  phht\n",
            "WARNING: word not in the vocabulary:  bluetit\n",
            "WARNING: word not in the vocabulary:  pchu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_node_features = next(iter(train_dataloader)).x.size(1)\n",
        "num_classes = 3\n",
        "model = GNNClassifier(input_dim=num_node_features, hidden_dim=64, output_dim=num_classes).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train\n",
        "for epoch in range(350):  # train for 50 epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.5f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JOIO4r_o2WJ",
        "outputId": "162cad60-b481-466c-c8b6-aca84afc0fc1"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.09032\n",
            "Epoch 50, Loss: 0.17506\n",
            "Epoch 100, Loss: 0.00253\n",
            "Epoch 150, Loss: 0.00099\n",
            "Epoch 200, Loss: 0.00059\n",
            "Epoch 250, Loss: 0.00039\n",
            "Epoch 300, Loss: 0.00028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "model.eval()\n",
        "for batch in dev_dataloader:\n",
        "  pred = model(batch).argmax(dim=1)\n",
        "  results = calc_metrics(batch.y, pred)\n",
        "  print(results)\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  pred = model(batch).argmax(dim=1)\n",
        "  results = calc_metrics(batch.y, pred)\n",
        "  print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NSmCceOrQe9",
        "outputId": "67af157e-236c-4e0d-e82a-662afe160074"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.4, 'f1': 0.2749354005167959, 'precision': 0.2707070707070707, 'recall': 0.2793650793650793, 'confusion_matrix': array([[12,  9,  0],\n",
            "       [ 8,  4,  3],\n",
            "       [ 2,  2,  0]])}\n",
            "{'accuracy': 0.575, 'f1': 0.49425287356321834, 'precision': 0.5416666666666666, 'recall': 0.476984126984127, 'confusion_matrix': array([[15,  6,  0],\n",
            "       [ 7,  7,  1],\n",
            "       [ 2,  1,  1]])}\n"
          ]
        }
      ]
    }
  ]
}