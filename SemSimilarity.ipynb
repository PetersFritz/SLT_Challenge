{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf1EiGN_EkKO",
        "outputId": "d873c65b-33e3-4628-c25e-882143ba73b7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Maps pos tags to tags used by WordNet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "RCtfXhxyErWc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve synset for word with specified POS tag\n",
        "def get_synset(word, pos):\n",
        "    wn_pos = get_wordnet_pos(pos)\n",
        "    if wn_pos:\n",
        "        synsets = wn.synsets(word, pos=wn_pos)\n",
        "        if synsets:\n",
        "            return synsets[0]\n",
        "    return None"
      ],
      "metadata": {
        "id": "3n-eLomFpUf5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity_edges(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Keep only nouns and pronouns (change this to whatever pos tags to include)\n",
        "    valid_tags = ('N', 'PRP', 'WP')\n",
        "    filtered = [(word, pos) for word, pos in tagged if pos.startswith(valid_tags)]\n",
        "\n",
        "    # Return empty dict if similarity cannot be computes (<2 words)\n",
        "    if len(filtered) < 2:\n",
        "        return {}\n",
        "\n",
        "    edges = {}\n",
        "    for i in range(len(filtered) - 1):\n",
        "        word1, pos1 = filtered[i]\n",
        "        word2, pos2 = filtered[i + 1]\n",
        "\n",
        "        syn1 = get_synset(word1, pos1)\n",
        "        syn2 = get_synset(word2, pos2)\n",
        "\n",
        "        if syn1 and syn2:\n",
        "            sim = syn1.wup_similarity(syn2)\n",
        "            edges[(word1.lower(), word2.lower())] = sim if sim is not None else 0.0\n",
        "        else:\n",
        "            edges[(word1.lower(), word2.lower())] = 0.0\n",
        "\n",
        "    return edges"
      ],
      "metadata": {
        "id": "CBxtWJY0FTgv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in dataframe\n",
        "df = pd.read_csv('/content/ASR transcripts - Process-train_manual_vs_asr.csv') #replace with file path"
      ],
      "metadata": {
        "id": "nhMQE_KmsJY7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to only keep patient speech and remove diarisation markers (Pat: and Oth:)\n",
        "\n",
        "def extract_patient_speech(text):\n",
        "    # Keep only lines that start with Pat:\n",
        "    patient_lines = re.findall(r'Pat:\\s*(.*?)(?=Pat:|Oth:|$)', text, flags=re.DOTALL)\n",
        "    # Join them into one cleaned string\n",
        "    return ' '.join(line.strip() for line in patient_lines)\n",
        "\n",
        "# Apply to the 'asr' transcripts\n",
        "df['asr_cleaned'] = df['asr'].apply(extract_patient_speech)"
      ],
      "metadata": {
        "id": "rlghWpUTua1I"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['similarities'] = df['asr_cleaned'].apply(get_similarity_edges)"
      ],
      "metadata": {
        "id": "qP7ISex9s6Ix"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}