{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "!pip install contractions\n",
        "import contractions\n",
        "!pip install python-Levenshtein\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "!pip install eng_to_ipa\n",
        "import eng_to_ipa as ipa\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import words\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Jf1EiGN_EkKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Maps pos tags to tags used by WordNet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "RCtfXhxyErWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animal_synset = wn.synset(\"animal.n.01\")\n",
        "\n",
        "def get_synset(word, pos, sft):\n",
        "    \"\"\"\n",
        "    Retrieve synset for word with specified POS tag\n",
        "    pick the animal meaning (e.g. pick Synset('wren.n.02') for bird rather than n.01 for the church)\n",
        "    if no animal meaning, pick the first meaning\n",
        "    \"\"\"\n",
        "    wn_pos = get_wordnet_pos(pos)\n",
        "    if wn_pos:\n",
        "        synsets = wn.synsets(word, pos=wn_pos)\n",
        "        if synsets:\n",
        "            if sft:\n",
        "                for syn in synsets:\n",
        "                    if animal_synset in syn.lowest_common_hypernyms(animal_synset):\n",
        "                        return syn\n",
        "                return synsets[0]\n",
        "            return synsets[0]\n",
        "    return None"
      ],
      "metadata": {
        "id": "3n-eLomFpUf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity_edges(row, remove_none_syns):\n",
        "    text = row['asr_cleaned']\n",
        "    filename = row['file']\n",
        "\n",
        "    tokens = word_tokenize(contractions.fix(text))\n",
        "    tagged = pos_tag(tokens)\n",
        "    tagged = [pair for pair in tagged if pair[0] not in string.punctuation]\n",
        "\n",
        "\n",
        "    # Keep only nouns and pronouns (change this to whatever pos tags to include)\n",
        "    # valid_tags = ('N', 'PRP', 'WP')\n",
        "    if 'SFT' in filename:\n",
        "        valid_tags = ('N')\n",
        "    else:\n",
        "        valid_tags = None\n",
        "\n",
        "    if valid_tags:\n",
        "        filtered = [(word, pos) for word, pos in tagged if pos.startswith(valid_tags)]\n",
        "    else:\n",
        "        filtered = [(word, pos) for word, pos in tagged]\n",
        "\n",
        "    sft = 'SFT' in filename\n",
        "\n",
        "    if remove_none_syns:\n",
        "        filtered_syns = [(word, get_synset(word, pos, sft)) for word, pos in filtered if get_synset(word, pos, sft) is not None]\n",
        "    else:\n",
        "        filtered_syns = [(word, get_synset(word, pos, sft)) for word, pos in filtered]\n",
        "\n",
        "    filtered_phons = [(word, ipa.convert(word)) for word, pos in filtered]\n",
        "\n",
        "    # Return empty dict if similarity cannot be computes (<2 words)\n",
        "    # !!\n",
        "    if len(filtered_syns) < 2:\n",
        "        return [], []\n",
        "\n",
        "    sem_edges = []\n",
        "    for i in range(len(filtered_syns) - 1):\n",
        "        word1, syn1 = filtered_syns[i]\n",
        "        word2, syn2 = filtered_syns[i + 1]\n",
        "\n",
        "        if syn1 and syn2:\n",
        "            sim = syn1.wup_similarity(syn2)\n",
        "            # sim is None if the pos of syn1 and syn2 are different, or there's no common ancestor in the tree\n",
        "            # this case is really rare - so far none in the train data\n",
        "            edge = (word1.lower(), word2.lower(), {'sem_similarity': sim})\n",
        "        else:\n",
        "            edge = (word1.lower(), word2.lower(), {'sem_similarity': None})\n",
        "        sem_edges.append(edge)\n",
        "\n",
        "    phon_edges = []\n",
        "    for i in range(len(filtered_phons) - 1):\n",
        "        word1, phon1 = filtered_phons[i]\n",
        "        word2, phon2 = filtered_phons[i + 1]\n",
        "\n",
        "        if phon1 and phon2:\n",
        "            sim = 1 - (levenshtein_distance(phon1, phon2) / max(len(phon1), len(phon2)))\n",
        "            edge = (word1.lower(), word2.lower(), {'phon_similarity': sim})\n",
        "        else:\n",
        "            edge = (word1.lower(), word2.lower(), {'phon_similarity': None})\n",
        "        phon_edges.append(edge)\n",
        "\n",
        "\n",
        "    values = [edge[2]['sem_similarity'] for edge in sem_edges if edge[2]['sem_similarity'] is not None]\n",
        "    if values:\n",
        "        average = sum(values) / len(values)\n",
        "\n",
        "        for edge in sem_edges:\n",
        "            word1, word2, attr = edge\n",
        "            if attr['sem_similarity'] is None:\n",
        "                attr['sem_similarity'] = average\n",
        "    else:\n",
        "        return get_similarity_edges(row, remove_none_syns=True)\n",
        "    return sem_edges, phon_edges"
      ],
      "metadata": {
        "id": "60EhMb2f3WcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity_edges_valid_only(row):\n",
        "    syns = row['valid_tokens']\n",
        "    phons = [(token, ipa.convert(token)) for token, syn in syns]\n",
        "\n",
        "    # Return empty dict if similarity cannot be computes (<2 words)\n",
        "    if len(syns) < 2:\n",
        "        return [], []\n",
        "\n",
        "    sem_edges = []\n",
        "    for i in range(len(syns) - 1):\n",
        "        word1, syn1 = syns[i]\n",
        "        word2, syn2 = syns[i + 1]\n",
        "\n",
        "        if syn1 and syn2:\n",
        "            sim = syn1.wup_similarity(syn2)\n",
        "            # sim is None if the pos of syn1 and syn2 are different, or there's no common ancestor in the tree\n",
        "            # this case is really rare - so far none in the train data\n",
        "            edge = (word1.lower(), word2.lower(), {'similarity': sim})\n",
        "        else:\n",
        "            edge = (word1.lower(), word2.lower(), {'similarity': None})\n",
        "        sem_edges.append(edge)\n",
        "\n",
        "    phon_edges = []\n",
        "    for i in range(len(phons) - 1):\n",
        "        word1, phon1 = phons[i]\n",
        "        word2, phon2 = phons[i + 1]\n",
        "\n",
        "        if phon1 and phon2:\n",
        "            sim = 1 - (levenshtein_distance(phon1, phon2) / max(len(phon1), len(phon2)))\n",
        "            edge = (word1.lower(), word2.lower(), {'similarity': sim})\n",
        "        else:\n",
        "            edge = (word1.lower(), word2.lower(), {'similarity': None})\n",
        "        phon_edges.append(edge)\n",
        "\n",
        "    sem_values = [edge[2]['similarity'] for edge in sem_edges if edge[2]['similarity'] is not None]\n",
        "    sem_average = sum(sem_values) / len(sem_values)\n",
        "\n",
        "    for edge in sem_edges:\n",
        "        word1, word2, attr = edge\n",
        "        if attr['similarity'] is None:\n",
        "            attr['similarity'] = sem_average\n",
        "\n",
        "    return sem_edges, phon_edges"
      ],
      "metadata": {
        "id": "Y91yFMWE5z9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(graph):\n",
        "    # 'sem_similarity' or 'phon_similarity'\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    pos = nx.spring_layout(graph, weight='similarity')  # Layout for positioning\n",
        "    nx.draw(\n",
        "        graph, pos, with_labels=True, node_color='lightblue', edgecolors='black',\n",
        "        node_size=1600, font_size=16, connectionstyle=\"arc3,rad=0.2\", arrows=True\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_graph(edges):\n",
        "    gr = nx.MultiDiGraph()\n",
        "    gr.add_edges_from(edges)\n",
        "    return gr\n",
        "\n",
        "def get_graph_info(graph):\n",
        "    res = {}\n",
        "    lengths = dict(nx.all_pairs_dijkstra_path_length(graph, weight='similarity'))\n",
        "    diameter = max(max(lengths[u].values()) for u in graph.nodes)\n",
        "    res['diameter'] = diameter\n",
        "\n",
        "    res['number_of_nodes'] = graph.number_of_nodes()\n",
        "    res['number_of_edges'] = graph.number_of_edges()\n",
        "    res['PE'] =  (np.array(list(Counter(graph.edges()).values()))>1).sum()\n",
        "\n",
        "    res['LCC'] =  nx.algorithms.components.number_weakly_connected_components(graph)\n",
        "    res['LSC'] =  nx.algorithms.components.number_strongly_connected_components(graph)\n",
        "\n",
        "    degrees = list(dict(graph.degree()).values())\n",
        "    res['degree_average'] =  np.mean(degrees)\n",
        "    res['degree_std'] =  np.std(degrees)\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "XCGwiXRqp0Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in dataframe\n",
        "df = pd.read_csv('/content/ASR transcripts - Process-train_manual_vs_asr.csv') #replace with file path\n",
        "df['id'] = df['file'].str.split('__').str[0]\n",
        "metadata = pd.read_csv('PROCESS_METADATA_ALL.csv')\n",
        "df = df.merge(metadata, left_on='id', right_on='anyon_IDs')"
      ],
      "metadata": {
        "id": "nhMQE_KmsJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to only keep patient speech and remove diarisation markers (Pat: and Oth:)\n",
        "\n",
        "def extract_patient_speech(text):\n",
        "    # Keep only lines that start with Pat:\n",
        "    patient_lines = re.findall(r'Pat:\\s*(.*?)(?=Pat:|Oth:|$)', text, flags=re.DOTALL)\n",
        "    # Join them into one cleaned string\n",
        "    return ' '.join(line.strip() for line in patient_lines)\n",
        "\n",
        "# Apply to the 'asr' transcripts\n",
        "df['asr_cleaned'] = df['asr'].apply(extract_patient_speech)"
      ],
      "metadata": {
        "id": "rlghWpUTua1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker()\n",
        "word_list = set(w.lower() for w in words.words())\n",
        "\n",
        "def get_vf_scores(row, task):\n",
        "    text = row['asr_cleaned']\n",
        "    filename = row['file']\n",
        "    tokens = word_tokenize(contractions.fix(text))\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    # print()\n",
        "    # print(row['asr_cleaned'])\n",
        "    # print(tokens)\n",
        "    real_tokens = [token for token in tokens if token.lower() in word_list or token in spell]\n",
        "    if task == 'PFT':\n",
        "        real_tokens_p = [token for token in real_tokens if token.lower().startswith('p')]\n",
        "        if row['id'] == 'Process-rec-114':\n",
        "            real_tokens_p = [token for token in real_tokens if token.lower().startswith('b')]\n",
        "        real_tokens_p_syns = []\n",
        "        for token in real_tokens_p:\n",
        "            syn = wn.synsets(token)\n",
        "            if syn:\n",
        "                real_tokens_p_syns.append((token, syn[0]))\n",
        "            else:\n",
        "                real_tokens_p_syns.append((token, None))\n",
        "\n",
        "        # print(real_tokens_p)\n",
        "        return real_tokens_p_syns, tokens, len(real_tokens_p), len(tokens), len(real_tokens_p)/len(tokens)\n",
        "    if task == 'SFT':\n",
        "        real_tokens_animal = []\n",
        "        for token in tokens:\n",
        "            synsets = wn.synsets(token, pos=wn.NOUN)\n",
        "            if synsets:\n",
        "                # is_animal = False\n",
        "                for syn in synsets:\n",
        "                    if animal_synset in syn.lowest_common_hypernyms(animal_synset):\n",
        "                        real_tokens_animal.append((token, syn))\n",
        "                        break\n",
        "                #         is_animal = True\n",
        "                # if is_animal:\n",
        "                #     real_tokens_animal.append(token)\n",
        "        # print(real_tokens_animal)\n",
        "        return real_tokens_animal, tokens, len(real_tokens_animal), len(tokens), len(real_tokens_animal)/len(tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m2o2ALzipwoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_similarity_edges_valid_only\n",
        "\n",
        "sft_df = df[df['file'].str.contains('SFT')]\n",
        "sft_df[['valid_tokens', 'tokens', 'n_valid_tokens', 'n_totoal_tokens', 'valid_token_ratio']] = sft_df.apply(lambda x: get_vf_scores(x, task='SFT'), axis=1, result_type='expand')\n",
        "\n",
        "pft_df = df[df['file'].str.contains('PFT')]\n",
        "pft_df[['valid_tokens', 'tokens', 'n_valid_tokens', 'n_totoal_tokens', 'valid_token_ratio']] = pft_df.apply(lambda x: get_vf_scores(x, task='PFT'), axis=1, result_type='expand')\n",
        "\n",
        "sft_df[['sem_edges', 'phon_edges']] = sft_df.apply(lambda x: get_similarity_edges_valid_only(x), axis=1, result_type='expand')\n",
        "sft_graph_info_sem = sft_df['sem_edges'].apply(lambda x: get_graph_info(get_graph(x))).apply(pd.Series)\n",
        "sft_graph_info_phon = sft_df['phon_edges'].apply(lambda x: get_graph_info(get_graph(x))).apply(pd.Series)\n",
        "\n",
        "pft_df[['sem_edges', 'phon_edges']] = pft_df.apply(lambda x: get_similarity_edges_valid_only(x), axis=1, result_type='expand')\n",
        "pft_graph_info_sem = pft_df['sem_edges'].apply(lambda x: get_graph_info(get_graph(x))).apply(pd.Series)\n",
        "pft_graph_info_phon = pft_df['phon_edges'].apply(lambda x: get_graph_info(get_graph(x))).apply(pd.Series)\n"
      ],
      "metadata": {
        "id": "8UUS3SutB7WM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_df = pd.concat([sft_df, sft_graph_info_sem.add_prefix('sem_'), sft_graph_info_phon.add_prefix('phon_')], axis=1)\n",
        "pft_df = pd.concat([pft_df, pft_graph_info_sem.add_prefix('sem_'), pft_graph_info_phon.add_prefix('phon_')], axis=1)\n",
        "pft_df_selected = pft_df[['id', 'valid_tokens', 'tokens',\n",
        "       'n_valid_tokens', 'n_totoal_tokens', 'valid_token_ratio', 'sem_edges',\n",
        "       'phon_edges', 'sem_diameter', 'sem_number_of_nodes',\n",
        "       'sem_number_of_edges', 'sem_PE', 'sem_LCC', 'sem_LSC',\n",
        "       'sem_degree_average', 'sem_degree_std', 'phon_diameter',\n",
        "       'phon_number_of_nodes', 'phon_number_of_edges', 'phon_PE', 'phon_LCC',\n",
        "       'phon_LSC', 'phon_degree_average', 'phon_degree_std']]\n",
        "merged_df = sft_df.merge(pft_df_selected, on='id', suffixes=('_sft', '_pft'))\n"
      ],
      "metadata": {
        "id": "-0owAQdMa_rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.columns"
      ],
      "metadata": {
        "id": "0BWFwPBGdh6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['label'] = merged_df['diagnosis'].map({'HC': 0, 'MCI': 1, 'Dementia': 2})"
      ],
      "metadata": {
        "id": "ue0yn1jRftTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "X = merged_df[['n_valid_tokens_sft', 'n_totoal_tokens_sft',\n",
        "       'valid_token_ratio_sft',\n",
        "       'sem_diameter_sft', 'sem_number_of_nodes_sft',\n",
        "       'sem_number_of_edges_sft', 'sem_PE_sft', 'sem_LCC_sft', 'sem_LSC_sft',\n",
        "       'sem_degree_average_sft', 'sem_degree_std_sft', 'phon_diameter_sft',\n",
        "       'phon_number_of_nodes_sft', 'phon_number_of_edges_sft', 'phon_PE_sft',\n",
        "       'phon_LCC_sft', 'phon_LSC_sft', 'phon_degree_average_sft',\n",
        "       'phon_degree_std_sft',\n",
        "       'n_valid_tokens_pft', 'n_totoal_tokens_pft', 'valid_token_ratio_pft',\n",
        "       'sem_diameter_pft',\n",
        "       'sem_number_of_nodes_pft', 'sem_number_of_edges_pft', 'sem_PE_pft',\n",
        "       'sem_LCC_pft', 'sem_LSC_pft', 'sem_degree_average_pft',\n",
        "       'sem_degree_std_pft', 'phon_diameter_pft', 'phon_number_of_nodes_pft',\n",
        "       'phon_number_of_edges_pft', 'phon_PE_pft', 'phon_LCC_pft',\n",
        "       'phon_LSC_pft', 'phon_degree_average_pft', 'phon_degree_std_pft']]\n",
        "y = merged_df['label']\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"\\n=== {clf_name} ===\")\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        if isinstance(clf, MultinomialNB):\n",
        "            X_train = np.abs(X_train)\n",
        "            X_test = np.abs(X_test)\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "        preds = clf.predict(X_test)\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_trues.extend(y_test)\n",
        "\n",
        "    print(classification_report(all_trues, all_preds))\n"
      ],
      "metadata": {
        "id": "0EC7oP5zeE9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_df_sem = pd.concat([sft_df, sft_graph_info_sem], axis=1)\n",
        "\n",
        "for col in sft_graph_info_sem.columns:\n",
        "    try:\n",
        "        sns.histplot(data=sft_df_sem, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'SFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6wwyYOb2VPyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_df_phon = pd.concat([sft_df, sft_graph_info_phon], axis=1)\n",
        "\n",
        "for col in sft_graph_info_phon.columns:\n",
        "    try:\n",
        "        sns.histplot(data=sft_df_phon, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'SFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "177PBJ-OWz6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, sub_group in pft_df.groupby('diagnosis'):\n",
        "    print(name)\n",
        "    for _, row in sub_group.iterrows():\n",
        "        graph = get_graph(row['phon_edges'])\n",
        "        plot_graph(graph)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZNvj_BrTIxZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sft_df['edges'].iloc[0].apply(lambda x: plot_graph(get_graph(x)))\n",
        "for name, sub_group in sft_df.groupby('diagnosis'):\n",
        "    print(name)\n",
        "    for _, row in sub_group.iterrows():\n",
        "        graph = get_graph(row['sem_edges'])\n",
        "        plot_graph(graph)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9kd20opxgCAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pft_df_phon = pd.concat([pft_df, pft_graph_info_phon], axis=1)\n",
        "\n",
        "for col in pft_graph_info_phon.columns:\n",
        "    try:\n",
        "        sns.histplot(data=pft_df_phon, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'PFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "J7Hj_3QrVwzv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pft_df_sem = pd.concat([pft_df, pft_graph_info_sem], axis=1)\n",
        "\n",
        "for col in pft_graph_info_sem.columns:\n",
        "    try:\n",
        "        sns.histplot(data=pft_df_sem, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'PFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "qCXoR5ioUxXr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['n_valid_tokens', 'n_totoal_tokens', 'valid_token_ratio']:\n",
        "    try:\n",
        "        sns.histplot(data=sft_df, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'SFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "a4RlmCDR14Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['n_valid_tokens', 'n_totoal_tokens', 'valid_token_ratio']:\n",
        "    try:\n",
        "        sns.histplot(data=pft_df, x=col, hue='diagnosis', kde=True, element='step', stat='density')\n",
        "        plt.title(f'PFT - Histogram of {col} by diagnosis')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "0FGA_c462XeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = sft_df.merge(pft_df[['id', 'n_valid_tokens']], on='id', suffixes=('_sft', '_pft'))\n",
        "merged_df['n_valid_tokens_diff'] = merged_df['n_valid_tokens_sft'] - merged_df['n_valid_tokens_pft']\n",
        "\n",
        "sns.histplot(data=merged_df, x='n_valid_tokens_diff', hue='diagnosis', kde=True, element='step', stat='density')\n",
        "plt.title(f'Histogram of n_valid_tokens_diff by diagnosis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-hslYyBY2tSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}